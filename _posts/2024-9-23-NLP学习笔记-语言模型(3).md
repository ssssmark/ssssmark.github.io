---
title: 'NLP学习笔记——语言模型(3)'
permalink: /posts/2024/09/NLP学习笔记-语言模型(3)
date: 2024-09-24
excerpt_separator: <!--more-->
tags:
    - NLP学习笔记
---
<!--more-->
# NLP学习笔记——语言模型(3)
## 1.前馈神经网络语言模型
与传统n元语言模型类似,前馈神经网络语言模型沿用了马尔可夫假设，即下一时刻的词只与过去n−1个词相关。前馈神经网络由三部分组成，分别为输入层、隐藏层和输出层。历史词序列首先经过输入层被转换为离散的独热编码，随后每个词的独热编码被映射为一个低维稠密的实数向量；隐藏层对词向量层的输出进行编码，进行多次线性变换与非线性映射；最后，隐藏层向量经过输出层被映射到词表空间，再利用Softmax 函数得到其词表上的概率分布。
输入层将由文本组成的词序列转化为模型可接受的低维稠密向量
$$v=[v_{(i-n+1)},...,v_{i-1}]$$
隐藏层公式：
$$h=f(W^{hid}v+b^{hid})$$
输出层的目标是基于隐藏层向量h得到词表空间V上的概率分布。
$$y=\mathrm{Softmax}(W^{out}h+b^{out})$$

## 2.循环神经网络模型