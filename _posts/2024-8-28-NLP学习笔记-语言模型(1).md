---
title: 'NLP学习笔记————语言模型(1)'
permalink: /posts/2024/09/NLP学习笔记-语言模型(1)
date: 2024-08-28
classes: wide
excerpt_separator: <!--more-->
tags: 
    - NLP学习笔记
---

<!--more-->
# NLP学习笔记——统计语言模型


**语言模型（Language Model，LM）** 的目标就是建模自然语言的概率分布。词汇表上的语言模型， 由函数 \\(P(w_1w_2...w_m)\\)表示，可以形式化地构建为词序列\\(w_1w_2...w_m \\)的概率分布,就是这个序列作为一个句子出现的可能性大小。

但是由于联合概率分布\\(P(w_1w_2...w_m)\\)​参数量巨大，很难直接计算，于是考虑用链式法则分解，根据句子从左往右的生成过程进行计算：
$$
P(w_1w_2...w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1w_2) · · · P(w_m|w_1w_2...w_{m−1})\\
=\prod_{i=1}^{m}P(w_i|w_1w_2 · · ·w_{i−1})
$$
由此可以依次进行生成，而不用一次性生成全部单词，但是\\(P(w_m|w_1w_2...w_{m−1})\\)的参数量依旧很大，我们可以进一步的对模型进行简化。
### 0.传统语言模型-词袋模型（Bag of Words）
词袋模型（Bag-of-Words model，BOW）最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子（因为里面装的都是词汇，所以称为词袋，Bag of words即因此而来），然后看这个袋子里装的都是些什么词汇，将其分类。



### 1.n元语言模型
在语言模型中,词\\(w_i\\)受到前\\(i-1\\)个单词的影响，将前\\(i-1\\)个词成为词\\(w_i\\)的历史，要预测词\\(w_i\\)，最简单的方法是根据语料库的统计特征进行预测，即根据词序列在语料库中的出现次数：
$$
P(w_i|w_1w_2 · · ·w_{i−1})=\frac{C(w_1w_2 · · ·w_{i})}{C(w_1w_2 · · ·w_{i−1})}
$$
其中\\(C(\cdot)\\)表示词序列在语料库出现次数。这是最大似然估计的方法，而当词序列足够长，这种估计的数据量就会成指数增长，称之为**维度灾难** 。
所以，为了解决这个问题，n-gram模型假设词\\(w_i\\)的出现只与前\\(n-1\\)个词有关。
$$
P(w_i|w_1w_2 · · ·w_{i−1})=P(w_i|w_{i-(n-1)}w_{i-(n-2)} · · ·w_{i−1})
\\
P(w_i|w_1^{i-1})=P(w_i|w_{n-i+1}^{i-1})
$$
n-gram表示由n 个连续单词构成的单元，也被称为\\(n\\)元语法单元。\\(n\\) 的取值越大，其历史信息越完整，但参数量也会随之增大。

尽管n元语言模型能缓解句子概率为0的问题，但语言是由人和时代创造的，具备无穷的可能性，再庞大的训练语料也无法覆盖所有的n-gram，而训练语料中的零频率并不代表零概率。因此，需要使用**平滑技术（Smoothing）**来解决这一问题，对所有可能出现的字符串都分配一个非零的概率值，从而避免零概率问题。平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为**数据平滑**（Data Smoothing）。平滑处理的基本思想是提高低概率，降低高概率，使整体的概率分布趋于均匀。
#### 1.1 加法平滑
**加法平滑（Additive Smoothing）**是实际运用中最
常用的平滑技术之一。其思想是假设事件出现的次数比实际出现的次数多\\(δ\\) 次。以二元语言模型为例，平滑后的条件概率为：
$$P(w_i|w_{i-1})=\frac{\delta+\mathrm{C}(w_{i-1}w_i)}{\sum_{w_i}\delta+\mathrm{C}(w_{i-1}w_i)}=\frac{\delta+\mathrm{C}(w_{i-1}w_i)}{\delta|\mathrm{V}|+\mathrm{C}(w_{i-1})}$$
拓展到n元模型
$$P(w_i|w_{i-n+1}^{i-1})=\frac{\delta+\mathrm{C}(w_{i-n+1}^i)}{\delta|\mathbb{V}|+\sum_{w_i}\mathrm{C}(w_{i-n+1}^i)}$$
针对所有不在词表中的单词，可以统一映射为一个特定的词，从而保证所有情况下都不存在非零概率
#### 1.2 古德-图灵估计法
该方法基于的核心思想是将一部分已知事件的概率分配给未见的事件。对于\\(n\\) 元语言模型来说，降低出现次数多的\\(n\\)元语法单元的概率，同时将剩余概率分配给未出现的\\(n \\)元语法单元。
具体来说，对于任意一个出现了\\(r\\) 次的\\(n\\)元语法单元，按照如下公式修改为出现了\\(r^∗\\)次
$$r^*=(r+1)\frac{n_{r+1}}{n_r}$$
\\(n_r\\)代表有\\(n_r\\)个\\(n\\)元语法单元在训练语料中出现了\\(r\\)次
对其进行归一化后，即可得到出现$r$次的$n$元语法概率:
$$
p_r=\frac{r^*}N
$$

古德-图灵估计法的缺点是其无法用于估计\\(n_r = 0\\)的\\(n\\)元语法概率，并且其不能用于高阶语言模型和低阶语言模型的结合，而高阶与低阶模型的结合通常能带来更好的平滑效果。但古德-图灵方法的思想简单普适，因此其往往是作为一种基本方法与其他的平滑方法结合。
#### 1.3 katz平滑
其在古德-图灵估计法的基础上引入了高阶模型与低阶模型的结合。Katz平滑法的基本思想是将因减值获得的概率余量根据低阶模型的分布分配给未见事件，而不是进行平均分配，从而令低概率事件有更合理的概率分布。
Katz平滑法的做法是，当事件在样本中出现的频次大于某一数值k 时，运用最大似然估计法，通过减值来估计其概率值；而当事件的频次小于\\(k\\)值时，使用低阶的语法模型作为代替高阶语法模型的后备。

#### 1.4 n-gram及平滑算法总结
$n$元语法模型整体上来看与训练语料规模和模型的阶数有较大的关系，不同的平滑算法在不同情况下的表现有较大的差距。平滑算法虽然较好的解决了零概率问题，但是基于稀疏表示的\\(n\\)元语言模型仍然有三个较为明显的缺点：
（1）无法建模长度超过\\(n\\)的上下文；
（2）依赖人工设计规则的平滑技术；
（3）当$n $增大时，数据的稀疏性随之增大，模型的参数量更是指数级增加，并且模型受到数据稀疏问题的影响，其参数难以被准确的学习。
这类方法通常称为统计语言模型。



