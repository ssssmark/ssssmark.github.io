---
title: NLP学习笔记——语言模型(2)
date: 2024-07-16 13:48:49
tags: ['NLP学习笔记']
categories:
    - 技术
---
# NLP学习笔记——分布式表示
随着深度神经网络（DNN）的发展，利用神经网络的语言模型拥有比$n$元语言模型更强的学习能力，能够对上下文进行更好的建模。以**词向量(Word Embedding)** 为代表的分布式语言模型也不断涌现。
## 1. 分布式表示
分布式表示(Distributed Representation)旨在将文本表示为低维空间下稠密的向量，并可以在低维空间中计算向量间的相似度关系，来表示语义相似关联。在分布式表示提出之前，大多用**独热编码(One-Hot)**作为自然语言的表示方法，其中每个维度代表某个单词是否在文中出现，独热编码的维度与词表大小一致，具有极高的稀疏性，并且无法计算语义相似度关系。
### 1.1 单词分布式表示
单词分布式表示将单词表示为定长低维稠密向量。有两个问题需要解决：
<ol>
<li>如何衡量单词语义相近</li>
<li>如何衡量表示相似</li>
</ol> 

早期词向量模型通过统计方法构建向量，而目前更多利用机器学习方法，通过对大量无标注文本进行自监督学习，来学习词向量表示。

#### 1.1.1 基于共现矩阵奇异值分解的词向量模型
分布式假设下，希望单词之间的相似度体现为两个词出现在相同上下文的频率。采用 **共现矩阵(Co-occurrence Matrix)** 的矩阵分解方法，特别是基于**奇异值分解(SVD)** 的分解方法，这种方法称为潜在**语义分析（Latent Semantic Analysis，LSA)** 模型。
![alt text](/img/co-occurrence_matrix.jpg)
对于上下文窗口长度$n$，取某一单词前后窗口为$n$的上下文，对窗口内每个单词与该单词进行共现计数，可以获得共现矩阵。
共现矩阵的每一行可以自然地当做对应词的向量表示，因为它的不同维度表示和各个单词的共现次数，共现矩阵提供的向量表示可以体现词语之间的相似度。然而，共现矩阵提供的词向量表示维度和词表大小相同，依然面临表示稀疏性的问题。
所以需要采用奇异值分解的方法得到词的低维稠密向量。
对于矩阵$A$,可以通过奇异值分解得到$A=U\Sigma V^T$，其中$U\in\mathbb{R}^{m\times m}$和$V\in\mathbb{R}^{n\times n}$是酉矩阵，$\Sigma\in\mathbb{R}^{m\times n}$是对角矩阵。而根据奇异值分解的性质，前10%的奇异值就占据了90%以上的奇异值之和，所以可以取前 $d$个奇异值作为词向量的低秩近似。
$$A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^T\approx U_{m\times k}\Sigma_{k\times k}V_{k\times n}^T$$
相对于词语的独热表示，基于共现矩阵奇异值分解的词向量模型初步解决了表示稀疏性的问题，且可以在一定程度上体现词语之间的相似度。
![alt text](/img/SVD.png)

#### 1.1.2 基于上下文单词预测词向量模型
论文[Efficient estimation of word representations in vector space.ICLR. 2013.](https://arxiv.org/pdf/1301.3781)提出了Word2vec方法，去除了非线性隐藏层，使用自监督的方式从大量无监督文本训练词表示模型。构建了两个非常简单的神经网络模型结构：连续词袋模型（Continuous Bag Of Words，CBOW）和跳字模型（Skip-Gram，SG），用于学习单词分布式表示。
##### Skip-Gram模型
模型的基本假设是文本中的词可以预测其上下文窗口内的其他词。对于文本的中心词，通过最大化上下文的词在给定中心词下的条件概率，对模型进行训练。
$$\mathcal{L}(w_1\ldots w_T)=-\sum_{t=1}^T\sum_{-n\leqslant i\leqslant n,i\neq0}\log P(w_{t+i}|w_t)$$
其中，$w_t$是中心词，$w_{t+i}$是中心词的上下文词，通过训练上下文词和中心词的词嵌入矩阵来分别得到词表中每个单词作为上下文词和中心词时的词向量。Skip-Gram 模型通过上下文词向量和中心词向量的相似度估计上下文词的出现概率，最后将训练好的中心词的词向量作为词的向量表示。
![alt text](/img/Skip_Gram+CBOW.png)
##### CBOW模型
CBOW模型假设文本中的词可以通过其在文本中的上下文词推导出来。模型考虑特定上下文时中心词的概率，CBoW 模型通过最大化中心词在上下文中出现的条件概率，将负对数形式的损失函数作为优化目标，来进行模型的训练。
$$\mathcal{L}(w_1\ldots w_T)=-\sum_{t=1}^T\log P(w_t|w_{t-n},\ldots,w_{t-1},w_{t+1},\ldots,w_{t+n})$$
用$U\in\mathbb{R}^{|\mathbb{V}|\times d}$表示中心词向量矩阵，用$V\in\mathbb{R}^{|\mathbb{V}|\times d}$ 表示上下文词向量矩阵，根据上下文词生成中心词的条件概率具体计算公式如下所示：
$$v_o=\frac1{2m}\sum_{-n\leqslant i\leqslant n,i\neq0}v_{c+i}\\P(w_c|w_{c-n},\ldots,w_{c-1},w_{c+1},\ldots,w_{c+n})=\frac{exp(u_c^Tv_o)}{\sum_{i\in\mathbb{V}}exp(u_i^Tv_o)}$$
其中$v_0$ 是平均上下文向量，用于计算和中心词的相似度，$u_c$ 是$w_c$用作中心词的表示，$u_i$是词表中每个词用作中心词的表示。

CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好.最理想情况下的实现，这两种方法训练迭代两个矩阵$W$和$W'$，之后在输出层采用softmax函数来计算输出各个词的概率。
但在实际应用中这种方法的训练开销很大，不具有很强的实用性，为了使得模型便于训练，有学者提出了**Hierarchical Softmax和Negative Sampling**两种改进方法。

##### Hierarchical Softmax
Hierarchical Softmax对原模型的改进主要有两点，
第一点是从输入层到隐藏层的映射，没有采用原先的与矩阵W相乘然后相加求平均的方法，而是直接对所有输入的词向量求和。假设输入的词向量为（0,1,0,0）和（0,0,0,1），那么隐藏层的向量为（0,1,0,1）。
第二点改进是采用哈夫曼树来替换了原先的从隐藏层到输出层的矩阵$W’$。哈夫曼树的叶节点个数为词汇表的单词个数$V$，一个叶节点代表一个单词，。非叶子节点n 的上下文使用向量$u_n$ 表示，并用于建模其左、右子树上每个词语出现在上下文窗口中的概率之和。而从根节点到该叶节点的路径确定了这个单词最终输出的词向量。
![alt text](/img/Hierarchical-Softmax.png)
具体来说，这棵哈夫曼树除了根结点以外的所有非叶节点中都含有一个由参数θ确定的sigmoid函数，不同节点中的θ不一样。训练时隐藏层的向量与这个sigmoid函数进行运算，根据结果进行分类，若分类为负类则沿左子树向下传递，编码为0；若分类为正类则沿右子树向下传递，编码为1。

##### Negative Sampling
以skip-gram为例，优化目标位损失函数使正确的上下文词$w_o$ 和中心词$w_c$ 之间的相似度
uTo
vc 尽量大，同时对于词表中每个不在上下文窗口内的词，使其和中心词的相似度$u_iv_c$ 尽量小,而后者要将词表中的每个词进行计算，开销巨大。负采样（Negative Sampling）将目标函数中全体词表范围的相似度计算修正为目标词和K 个负例的相似度计算，其中K 是远小于词表大小的超参数。通过这种方式，使得训练的计算开销与词表大小无关，而只与超参数K相关。
在优化目标方面，对于每组上下文词-中心词，原始的Skip-Gram 模型只优化$P(D=1|w_o,w_c)$的正样本，而负采样下的模型从原始词分布$P(w)$中采样K个未出现在上下文窗口中的词$w_1, . . . ,w_K$作为负样本，并额外将$P(D=0|w_i,w_c)$ 作为优化目标，形式化表示为
$$P(w_o|w_c)=P(D=1|w_o,w_c)\prod_{i=1}^KP(D=0|w_i,w_c)$$